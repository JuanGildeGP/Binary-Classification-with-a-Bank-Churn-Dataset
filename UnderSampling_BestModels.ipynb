{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/df.csv')\n",
    "\n",
    "df = df.drop(['CustomerId', 'Surname','id'], axis=1)\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "under= RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "X_train_under, y_train_under = under.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters found for each model\n",
    "params_lgb = {'boosting_type': 'gbdt', 'colsample_bytree': 0.8, 'learning_rate': 0.1, 'n_estimators': 100, 'num_leaves': 15, 'subsample': 0.8}\n",
    "params_xgboost = {'colsample_bytree': 0.8, 'gamma': 0, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.95}\n",
    "params_gradient_boosting = {'learning_rate': 0.1, 'max_depth': 5, 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 100, 'subsample': 0.8}\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(**params_lgb)\n",
    "xgboost_model = XGBClassifier(**params_xgboost)\n",
    "gradient_boosting_model = GradientBoostingClassifier(**params_gradient_boosting)\n",
    "\n",
    "models = [('lgb', lgb_model), ('xgboost', xgboost_model), ('gradient_boosting', gradient_boosting_model)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 24445, number of negative: 91078\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003985 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 115523, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211603 -> initscore=-1.315291\n",
      "[LightGBM] [Info] Start training from score -1.315291\n",
      "Ensemble ROC AUC Score: 0.8881266936551053\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_model = VotingClassifier(estimators=models, voting='soft')  # 'soft' for averaging probabilities\n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "y_ensemble_probabilities = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc_ensemble = roc_auc_score(y_test, y_ensemble_probabilities)\n",
    "\n",
    "print(f\"Ensemble ROC AUC Score: {roc_auc_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 24445, number of negative: 91078\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 115523, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211603 -> initscore=-1.315291\n",
      "[LightGBM] [Info] Start training from score -1.315291\n",
      "[LightGBM] [Info] Number of positive: 19556, number of negative: 72862\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001309 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 92418, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315285\n",
      "[LightGBM] [Info] Start training from score -1.315285\n",
      "[LightGBM] [Info] Number of positive: 19556, number of negative: 72862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003165 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 859\n",
      "[LightGBM] [Info] Number of data points in the train set: 92418, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315285\n",
      "[LightGBM] [Info] Start training from score -1.315285\n",
      "[LightGBM] [Info] Number of positive: 19556, number of negative: 72862\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003460 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 92418, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211604 -> initscore=-1.315285\n",
      "[LightGBM] [Info] Start training from score -1.315285\n",
      "[LightGBM] [Info] Number of positive: 19556, number of negative: 72863\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001086 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 860\n",
      "[LightGBM] [Info] Number of data points in the train set: 92419, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211602 -> initscore=-1.315299\n",
      "[LightGBM] [Info] Start training from score -1.315299\n",
      "[LightGBM] [Info] Number of positive: 19556, number of negative: 72863\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003437 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 858\n",
      "[LightGBM] [Info] Number of data points in the train set: 92419, number of used features: 12\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.211602 -> initscore=-1.315299\n",
      "[LightGBM] [Info] Start training from score -1.315299\n",
      "Ensemble ROC AUC Score: 0.8878418776425323\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ensemble_model = StackingClassifier(estimators=models)  \n",
    "\n",
    "ensemble_model.fit(X_train, y_train)\n",
    "\n",
    "y_ensemble_probabilities = ensemble_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "roc_auc_ensemble = roc_auc_score(y_test, y_ensemble_probabilities)\n",
    "\n",
    "print(f\"Ensemble ROC AUC Score: {roc_auc_ensemble}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
